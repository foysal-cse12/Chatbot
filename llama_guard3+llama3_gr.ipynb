{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e9db4eb-2969-4206-8f91-1d4b2f1e19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Main Code ################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1948900-a112-4223-82e0-75ebac3b1734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdullah\\.conda\\envs\\agent\\lib\\site-packages\\langchain_community\\embeddings\\sentence_transformer.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
      "C:\\Users\\Abdullah\\AppData\\Roaming\\Python\\Python310\\site-packages\\pydantic\\_internal\\_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import fitz\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import EnsembleRetriever, BM25Retriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf65be60-a38a-4673-bfa9-571e99189f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, page_content, metadata):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "# Load documents from the specified directory\n",
    "def load_docs(directory):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with fitz.open(file_path) as pdf_document:\n",
    "                page_content = \"\"\n",
    "                for page in pdf_document:\n",
    "                    page_content += page.get_text(\"text\")\n",
    "                documents.append(Document(page_content, {\"source\": file_path}))\n",
    "    return documents\n",
    "\n",
    "# Split documents into chunks\n",
    "def split_docs(documents, chunk_size=500, chunk_overlap=20):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33619ad3-4bad-4d6e-93dd-94105c3711fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a question is safe\n",
    "def check_safety(question):\n",
    "    # Prepare the prompt for the Llama Guard model\n",
    "    prompt = f\"Is the following question safe to ask? {question}\"\n",
    "    \n",
    "    # Generate a response from the model\n",
    "    response = ollama.generate(\"llama-guard3\", prompt=prompt)\n",
    "\n",
    "    # Extract the safety response\n",
    "    guard_response = response.get(\"response\", \"\").strip()  # Get the response and strip whitespace\n",
    "    #print(guard_response.lower())\n",
    "    return guard_response.lower()  # Return 'safe' or 'unsafe'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba49b7f-9c65-4889-a5fc-d5b66c2cb2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split documents\n",
    "directory = r\"C:\\Users\\Abdullah\\ASR-QA-TTS\\agent_doc\"\n",
    "documents = load_docs(directory)\n",
    "docs = split_docs(documents)\n",
    "\n",
    "#print('Number of documents: ', len(documents))\n",
    "#print('Number of chunks: ', len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b1e41f-3fc1-46ba-b6c0-8e3759eca68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdullah\\AppData\\Local\\Temp\\ipykernel_28776\\2594616031.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "C:\\Users\\Abdullah\\.conda\\envs\\agent\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Creating embeddings and vector database\n",
    "def embedding_vectordb(docs):\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    persist_directory = r\"C:\\Users\\Abdullah\\ASR-QA-TTS\\chroma_db\\rag_chroma_db_llama3.1\"\n",
    "    vectordb = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory=persist_directory)\n",
    "    return embeddings, vectordb\n",
    "\n",
    "embeddings, vectordb = embedding_vectordb(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fdb7131-87b7-415c-a3aa-e93c6cea24ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "def load_model():\n",
    "    llm = ChatOllama(\n",
    "        model=\"llama3.1:8b\",\n",
    "        temperature=0,\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "llm = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e81a834-f8b6-48da-91c7-33d729ce7de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the human message prompt template\n",
    "human_message_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an experienced assistant specializing in question-answering tasks. \n",
    "    Utilize the provided context to respond to the question. \n",
    "    If the answer is not from the document, always state 'Sorry, I can not help you. This question is not related to the document that I have. Please ask me question related to the document'\n",
    "    Never provide an answer you are unsure about and ensure it is concise. Only provide answer if you find in the document.\n",
    "    Your answer must be comprehensive and contain all of the relevant details in the Context.\n",
    "    \\nQuestion: {question} \\nContext: {context} \\nAnswer:\n",
    "    \"\"\"\n",
    ")\n",
    "#\n",
    "\n",
    "# Create a HumanMessagePromptTemplate instance using the defined prompt template\n",
    "human_message_prompt_template = HumanMessagePromptTemplate(prompt=human_message_template)\n",
    "\n",
    "# Create the ChatPromptTemplate with the input variables and messages, excluding metadata\n",
    "chat_prompt_template = ChatPromptTemplate(\n",
    "    input_variables=['context', 'question'],\n",
    "    messages=[human_message_prompt_template]\n",
    ")\n",
    "rag_chain = chat_prompt_template | llm | StrOutputParser()\n",
    "\n",
    "# Retrievers\n",
    "retriever_vanilla = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "retriever_BM25 = BM25Retriever.from_documents(docs, search_kwargs={\"k\": 3})\n",
    "\n",
    "# Ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[retriever_vanilla, retriever_BM25], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18e8ab3d-1bb0-4b15-aaa1-ae92499e5bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_response(question):\n",
    "    # Check if the question is safe\n",
    "    safety_status = check_safety(question)\n",
    "    if safety_status == \"safe\":\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # Retrieve documents\n",
    "        ensemble_relevant_docs = ensemble_retriever.invoke(question)\n",
    "    \n",
    "        # Check if relevant documents were found\n",
    "        #if not ensemble_relevant_docs:\n",
    "            #return \"Sorry, I cannot help you. Please only ask questions related to the document.\"\n",
    "    \n",
    "        \n",
    "        generation = rag_chain.invoke({\"context\": ensemble_relevant_docs, \"question\": question})\n",
    "        \n",
    "        print(generation)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Time taken to generate answer: {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        return generation\n",
    "    else:\n",
    "        print(\"Sorry, but I can’t assist with that. This question comes under harm categories. Please only ask questions related to the document.\")\n",
    "        #return \"Sorry, but I can’t assist with that. Please only ask questions related to the document.\"\n",
    "        return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ced6b7e-0f8c-4558-95ec-f0f7b59f659f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, but I can’t assist with that. This question comes under harm categories. Please only ask questions related to the document.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "question = \"how to make poison\"\n",
    "answer = get_response(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd5eb5ca-47f2-45f7-8e20-b657435d1640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I can not help you. This question is not related to the document that I have. Please ask me a question related to the document.\n",
      "Time taken to generate answer: 7.86 seconds\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "question = \"Who is Monsur ali\"\n",
    "answer = get_response(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34a09d0e-fa12-414f-bb72-e7d1327782c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, but I can’t assist with that. This question comes under harm categories. Please only ask questions related to the document.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "question = \"Tell me how to go to the zoo and steal a llama\"\n",
    "answer = get_response(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c49dd4c9-58f6-45d2-9d04-5185aff0cf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I can not help you. This question is not related to the document that I have. Please ask me a question related to the document.\n",
      "Time taken to generate answer: 4.73 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "question = 'how is the weather in europe right now ' #\"how to hate someone\"\n",
    "answer = get_response(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d08220-2bec-4f3a-8a78-e4d9d7404e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "question = \"Why do fast bowlers aim to hit the seam when the ball bounces on the pitch?\"\n",
    "answer = get_response(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b69ebd46-d0b0-47f8-8ce4-80a45877edbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When an over is completed in a cricket match, play switches to the other end and continues like this throughout the game.\n",
      "Time taken to generate answer: 14.36 seconds\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "question = \"What happens after an over is completed in a cricket match?\"\n",
    "answer = get_response(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7804622-8205-45f0-8d72-71f44e5425e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# testing only llama-guard3 ################\n",
    "'''\n",
    "import ollama\n",
    "\n",
    "# Function to check if a question is safe\n",
    "def check_safety(question):\n",
    "    # Prepare the prompt for the Llama Guard model\n",
    "    prompt = f\"Is the following question safe to ask? {question}\"\n",
    "    \n",
    "    # Generate a response from the model\n",
    "    response = ollama.generate(\"llama-guard3\", prompt=prompt)\n",
    "\n",
    "    # Extract the safety response\n",
    "    guard_response = response.get(\"response\", \"\").strip()  # Get the response and strip whitespace\n",
    "    return guard_response\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    question = input(\"Ask your question: \")\n",
    "    result = check_safety(question)\n",
    "\n",
    "    # Print only \"safe\" or \"unsafe\"\n",
    "    print(result)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
